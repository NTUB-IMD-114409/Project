import os
import chardet
import streamlit as st
from langchain_community.chat_models import ChatOllama
from langchain_community.vectorstores import Chroma
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

# âœ… Streamlit åŸºæœ¬è¨­å®š
st.set_page_config(page_title="ğŸ¦™ LLaMA3 æœ¬åœ°å•ç­”æ©Ÿå™¨äºº", layout="wide")
st.title("ğŸ¦™ æœ¬åœ° LLaMA3 + å‘é‡æœå°‹å•ç­”æ©Ÿå™¨äºº")

# âœ… æª”æ¡ˆä¸Šå‚³ä»‹é¢
uploaded_file = st.file_uploader("ğŸ“„ è«‹ä¸Šå‚³ .txt æª”æ¡ˆ", type=["txt"])

if uploaded_file:
    # âœ… åµæ¸¬æª”æ¡ˆç·¨ç¢¼
    raw = uploaded_file.read()
    encoding = chardet.detect(raw)["encoding"]
    text = raw.decode(encoding)

    # âœ… åˆ†æ®µè™•ç†ï¼ˆä¸­æ–‡æ¨™é»ï¼‰
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ"]
    )
    docs = splitter.create_documents([text])

    # âœ… å‘é‡ Embeddingï¼ˆOpenAIï¼Œå¯æ”¹ HuggingFaceï¼‰
    os.environ["OPENAI_API_KEY"] = ""
    embeddings = OpenAIEmbeddings()

    # âœ… å»ºç«‹å‘é‡è³‡æ–™åº«ï¼ŒåŠ å…¥ persist_directory ä¿®æ­£éŒ¯èª¤
    vectorstore = Chroma.from_documents(
        docs,
        embedding=embeddings,
        persist_directory="./chroma_db"
    )
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # âœ… æœ¬åœ°æ¨¡å‹ï¼šLLaMA3 (Ollama)
    llm = ChatOllama(model="llama3")
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    # âœ… å¼·åŒ–ç‰ˆ Prompt + Chainï¼ˆä¿è­‰ç”¨åˆ° contextï¼‰
    prompt = PromptTemplate.from_template("""
æ ¹æ“šä»¥ä¸‹å…§å®¹å›ç­”å•é¡Œï¼Œå¦‚æœæ‰¾ä¸åˆ°ç­”æ¡ˆå°±èªªã€Œç„¡æ³•å¾æ•…äº‹ä¸­æ‰¾åˆ°ç­”æ¡ˆã€ã€‚
=========
{context}
=========
å•é¡Œï¼š{question}
å›ç­”ï¼š
""")

    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    # âœ… èŠå¤©è¨˜æ†¶
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # âœ… ä½¿ç”¨è€…è¼¸å…¥
    user_input = st.chat_input("è«‹è¼¸å…¥ä½ çš„å•é¡Œ")
    if user_input:
        with st.spinner("LLaMA3 æ€è€ƒä¸­..."):
            result = chain.invoke(user_input)
            st.session_state.chat_history.append((user_input, result))

    # âœ… é¡¯ç¤ºå°è©±æ­·å²
    for q, a in st.session_state.chat_history:
        with st.chat_message("ä½ "):
            st.markdown(q)
        with st.chat_message("LLaMA3"):
            st.markdown(a)

else:
    st.info("â¬†ï¸ è«‹å…ˆä¸Šå‚³ä¸€å€‹ .txt æª”æ¡ˆä¾†é–‹å§‹å°è©±")
